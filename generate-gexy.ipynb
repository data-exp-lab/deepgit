{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94cfb9fd-21b3-42f6-9aa8-25fe3d606b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gllms/Flue jerosoler/Drawflow {'javascript', 'flowchart'}\n",
      "gllms/Flue sacode387/FlowRun {'executable-flowcharts', 'flowchart'}\n",
      "nevalang/neva cross-platform/dspatcher {'reactive-programming', 'fbp'}\n",
      "nevalang/neva hlang-tech/hlang {'flow-based', 'fbp-runtime'}\n",
      "nevalang/neva cross-platform/dspatch {'reactive-programming', 'fbp'}\n",
      "nevalang/neva cross-platform/dspatchables {'reactive-programming', 'fbp'}\n",
      "nevalang/neva flydelabs/flyde {'reactive-programming', 'flow-based'}\n",
      "nevalang/neva samuelmtimbo/unit {'reactive-programming', 'functional-programming', 'programming-language'}\n",
      "nevalang/neva fbbdev/nodal {'dataflow-compiler', 'compiler'}\n",
      "nevalang/neva ERnsTL/flowd {'fbp', 'fbp-runtime'}\n",
      "BugelNiels/nitro ern0/dataflow-editor-concept {'editor', 'dataflow'}\n",
      "BugelNiels/nitro cross-platform/dspatcher {'cpp', 'dataflow'}\n",
      "BugelNiels/nitro houmain/QML-NodeGraph {'graph', 'node'}\n",
      "BugelNiels/nitro Nicolas-Constanty/Dnai.Editor {'editor', 'dataflow', 'graph'}\n",
      "BugelNiels/nitro cross-platform/dspatch {'cpp', 'dataflow'}\n",
      "BugelNiels/nitro zenustech/zeno {'cpp', 'graphics'}\n",
      "BugelNiels/nitro cross-platform/dspatchables {'cpp', 'dataflow'}\n",
      "BugelNiels/nitro jerosoler/Drawflow {'editor', 'dataflow'}\n",
      "BugelNiels/nitro miroiu/nodify {'editor', 'graph'}\n",
      "BugelNiels/nitro imengyu/node-blueprint {'editor', 'node'}\n",
      "BugelNiels/nitro fbbdev/nodal {'cpp', 'dataflow', 'graph'}\n",
      "ern0/dataflow-editor-concept jerosoler/Drawflow {'editor', 'dataflow'}\n",
      "jerosoler/drawflow-minimap-example jerosoler/Drawflow {'dataflow', 'drawflow'}\n",
      "cross-platform/dspatcher fbbdev/nodal {'cpp', 'dataflow'}\n",
      "cross-platform/dspatcher ERnsTL/flowd {'dataflow', 'fbp'}\n",
      "hlang-tech/hlang jerosoler/Drawflow {'javascript', 'flow'}\n",
      "houmain/QML-NodeGraph newcat/baklavajs {'graph', 'node'}\n",
      "Nicolas-Constanty/Dnai.Editor jerosoler/Drawflow {'editor', 'dataflow'}\n",
      "Nicolas-Constanty/Dnai.Editor newcat/baklavajs {'editor', 'dataflow', 'graph'}\n",
      "cross-platform/dspatch fbbdev/nodal {'cpp', 'dataflow'}\n",
      "cross-platform/dspatch ERnsTL/flowd {'dataflow', 'fbp'}\n",
      "cross-platform/dspatchables fbbdev/nodal {'cpp', 'dataflow'}\n",
      "cross-platform/dspatchables ERnsTL/flowd {'dataflow', 'fbp'}\n",
      "jerosoler/Drawflow moonrailgun/codeck {'nodebased', 'flowchart'}\n",
      "jerosoler/Drawflow fbbdev/nodal {'flow', 'dataflow'}\n",
      "jerosoler/Drawflow newcat/baklavajs {'editor', 'flow', 'dataflow'}\n",
      "XBagon/PIPES caliconiko/shapes {'2d-programming-language', 'esoteric-programming-language', 'esoteric-language'}\n",
      "miroiu/nodify newcat/baklavajs {'editor', 'graph'}\n",
      "lambdabricks/bricks-front-react samuelmtimbo/unit {'live-programming', 'functional-programming'}\n",
      "FlowFuse/flowfuse node-red/node-red {'low-code', 'node-red'}\n",
      "moonrailgun/codeck DRUMNICORN/Visio {'low-code', 'flowchart'}\n",
      "frankframework/frank-flow newcat/baklavajs {'editor', 'flow'}\n",
      "fbbdev/nodal newcat/baklavajs {'flow', 'dataflow', 'graph'}\n",
      "GEXF file created: temp/logic_repos.gexf\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import ast\n",
    "from itertools import combinations\n",
    "\n",
    "similar_topics = [\n",
    "    \"flow-based-programming\",\n",
    "    \"visual-programming\",\n",
    "    \"visual-programming-editor\",\n",
    "    \"visual-programming-language\",\n",
    "    \"dataflow-programming\",\n",
    "    \"blockly\",\n",
    "    \"graph-editor\",\n",
    "    \"node-editor\"\n",
    "]\n",
    "\n",
    "df_repos = pd.read_csv(\"temp/tag_repos.csv\")\n",
    "\n",
    "# Create an empty undirected graph.\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes: Each repo becomes a node, and its topics are stored as a node attribute.\n",
    "for idx, row in df_repos.iterrows():\n",
    "    repo = row['repo_name']\n",
    "    topics = row['topics']\n",
    "    # Ensure topics are properly parsed from string representation of lists\n",
    "    if isinstance(topics, str) and topics.startswith(\"[\") and topics.endswith(\"]\"):\n",
    "        topics = ast.literal_eval(topics)  # Convert string list to actual list\n",
    "    elif isinstance(topics, str):\n",
    "        topics = [topics]  # Convert single topic string to list\n",
    "    \n",
    "    topics_cleaned = [t.strip().lower() for t in topics]  # Clean topics\n",
    "    topics_str = ','.join(topics_cleaned)\n",
    "    G.add_node(repo, topics=topics_str)\n",
    "\n",
    "# Add edges: Check for shared tags between repos and add an edge with weight equal to shared topics count in similar_topics.\n",
    "repos = list(G.nodes)\n",
    "for repo1, repo2 in combinations(repos, 2):\n",
    "    topics1 = set(G.nodes[repo1]['topics'].split(','))\n",
    "    topics2 = set(G.nodes[repo2]['topics'].split(','))\n",
    "    shared_topics = topics1 & topics2  # Ensure lowercase comparison\n",
    "    relevant_shared_topics = shared_topics & set(similar_topics)\n",
    "    non_relevant_shared_topics = shared_topics - set(similar_topics)\n",
    "    if len(relevant_shared_topics)>=2 & len(non_relevant_shared_topics)>=1:\n",
    "        print(repo1, repo2, non_relevant_shared_topics)\n",
    "        G.add_edge(repo1, repo2)\n",
    "\n",
    "# Write the graph to a GEXF file.\n",
    "output_file = \"temp/logic_repos.gexf\"\n",
    "nx.write_gexf(G, output_file)\n",
    "print(f\"GEXF file created: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "234440b7-24fd-467a-88ae-696c84792430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEXF file created: temp/logic_topics.gexf\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import ast\n",
    "from itertools import combinations\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "df_repos = pd.read_csv(\"temp/tag_repos.csv\")\n",
    "\n",
    "# Count topic frequencies\n",
    "topic_counter = Counter()\n",
    "topic_pairs = defaultdict(set)  # Dictionary to track topic pairs and the repos they appear in\n",
    "\n",
    "for idx, row in df_repos.iterrows():\n",
    "    topics = row['topics']\n",
    "    if isinstance(topics, str) and topics.startswith(\"[\") and topics.endswith(\"]\"):\n",
    "        topics = ast.literal_eval(topics)  # Convert string list to actual list\n",
    "    elif isinstance(topics, str):\n",
    "        topics = [topics]  # Convert single topic string to list\n",
    "    \n",
    "    topics_cleaned = {t.strip().lower() for t in topics}\n",
    "    topic_counter.update(topics_cleaned)\n",
    "    \n",
    "    for topic1, topic2 in combinations(topics_cleaned, 2):\n",
    "        topic_pairs[(topic1, topic2)].add(idx)  # Store the repo index where the pair appears\n",
    "\n",
    "# Filter topics with frequency greater than 5\n",
    "filtered_topics = {topic for topic, count in topic_counter.items() if count > 5}\n",
    "\n",
    "# Create an empty undirected graph.\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add edges between topics appearing in at least two same repositories and in the filtered set\n",
    "for (topic1, topic2), repos in topic_pairs.items():\n",
    "    if len(repos) >= 10 and topic1 in filtered_topics and topic2 in filtered_topics:\n",
    "        G.add_edge(topic1, topic2)\n",
    "\n",
    "# Add filtered topics as nodes\n",
    "G.add_nodes_from(filtered_topics)\n",
    "\n",
    "# Write the graph to a GEXF file.\n",
    "output_file = \"temp/logic_topics.gexf\"\n",
    "nx.write_gexf(G, output_file)\n",
    "print(f\"GEXF file created: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2748fff4-5481-419e-ba4b-396c6d9bcae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dominant Topic for Each of the Top 10 Clusters ===\n",
      "Cluster 0: language\n",
      "Cluster 1: javascript\n",
      "Cluster 2: artificial-intelligence\n",
      "Cluster 3: declarative-language\n",
      "Cluster 4: constraint-programming\n",
      "Cluster 5: machine-learning\n",
      "Cluster 6: rdf\n",
      "Cluster 7: interpreter\n",
      "Cluster 8: parser\n",
      "Cluster 9: prolog-exercises\n",
      "GEXF file created: temp/logic_repos_with_clusters.gexf\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import ast\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "from networkx.algorithms import community\n",
    "\n",
    "# Define similar topics\n",
    "similar_topics = {\n",
    "'swi-prolog',\n",
    " 'prolog-implementation',\n",
    " 'prolog',\n",
    " 'prolog-programming-language',\n",
    " 'logic-programming',\n",
    " 'logic',\n",
    " 'answer-set-programming',\n",
    " 'declarative-programming'\n",
    " 'datalog'\n",
    " 'iso-prolog-standard',\n",
    " 'logical-programming',\n",
    " 'prolog-application'\n",
    "}\n",
    "\n",
    "# Load repository data\n",
    "df_repos = pd.read_csv(\"temp/logic_tag_repos.csv\")\n",
    "\n",
    "# Create an undirected graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes with topics as attributes\n",
    "for idx, row in df_repos.iterrows():\n",
    "    repo = row['repo_name']\n",
    "    topics = row['topics']\n",
    "\n",
    "    if isinstance(topics, str) and topics.startswith(\"[\") and topics.endswith(\"]\"):\n",
    "        topics = ast.literal_eval(topics)  # Convert string list to actual list\n",
    "    elif isinstance(topics, str):\n",
    "        topics = [topics]  # Convert single topic string to list\n",
    "\n",
    "    topics_cleaned = {t.strip().lower() for t in topics}  # Clean and deduplicate topics\n",
    "    G.add_node(repo, topics=\",\".join(sorted(topics_cleaned)))  # Convert set to a sorted string\n",
    "\n",
    "# Add edges based on shared topics\n",
    "repos = list(G.nodes)\n",
    "for repo1, repo2 in combinations(repos, 2):\n",
    "    topics1 = set(G.nodes[repo1]['topics'].split(\",\"))\n",
    "    topics2 = set(G.nodes[repo2]['topics'].split(\",\"))\n",
    "\n",
    "    shared_topics = topics1 & topics2\n",
    "    relevant_shared_topics = shared_topics & similar_topics\n",
    "    non_relevant_shared_topics = shared_topics - similar_topics\n",
    "\n",
    "    if (len(relevant_shared_topics) >= 2) and (len(non_relevant_shared_topics) >= 1):\n",
    "        G.add_edge(repo1, repo2, shared_topics=\",\".join(sorted(non_relevant_shared_topics)))\n",
    "\n",
    "# Perform Louvain community detection (clusters repos by non-similar topics)\n",
    "louvain_communities = community.louvain_communities(G, weight=None, resolution=1.0)\n",
    "top_clusters = sorted(louvain_communities, key=len, reverse=True)[:10]  # Select top 10 largest clusters\n",
    "\n",
    "# Extract dominant topics from each of the top 10 clusters\n",
    "cluster_topics = {}\n",
    "\n",
    "for i, cluster in enumerate(top_clusters):\n",
    "    topic_counter = Counter()\n",
    "    \n",
    "    for repo in cluster:\n",
    "        non_similar_topics = set(G.nodes[repo]['topics'].split(\",\")) - similar_topics\n",
    "        topic_counter.update(non_similar_topics)\n",
    "\n",
    "    # Get the **most common topic** in this cluster (dominant topic)\n",
    "    dominant_topic = topic_counter.most_common(1)[0][0] if topic_counter else \"Unknown\"\n",
    "\n",
    "    # Store the cluster's dominant topic\n",
    "    cluster_topics[i] = dominant_topic  \n",
    "\n",
    "    # Assign the dominant topic as the category for each node in the cluster\n",
    "    for repo in cluster:\n",
    "        G.nodes[repo][\"category\"] = dominant_topic  # Assign dominant topic as category\n",
    "\n",
    "# Print Cluster Topics\n",
    "print(\"\\n=== Dominant Topic for Each of the Top 10 Clusters ===\")\n",
    "for cluster_id, topic in cluster_topics.items():\n",
    "    print(f\"Cluster {cluster_id}: {topic}\")\n",
    "\n",
    "# Write the graph to a GEXF file\n",
    "output_file = \"temp/logic_repos_with_clusters.gexf\"\n",
    "nx.write_gexf(G, output_file)\n",
    "print(f\"GEXF file created: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80644cf0-a91b-4c40-aed5-e599f30007e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
