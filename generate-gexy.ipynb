{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94cfb9fd-21b3-42f6-9aa8-25fe3d606b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CapelliC/hhprolog CapelliC/hitchhicker-prolog {'javascript', 'virtual-machine'}\n",
      "retrofor/iamai lab-v2/pyreason {'machine-learning', 'python'}\n",
      "retrofor/iamai lps-js/lps-studio {'artificial-intelligence', 'ai'}\n",
      "trealla-prolog/trealla tau-prolog/tau-prolog {'iso-prolog-standard', 'prolog-interpreter'}\n",
      "trealla-prolog/trealla ciao-lang/ciao {'iso-prolog-standard', 'prolog-interpreter'}\n",
      "trealla-prolog/trealla mthom/scryer-prolog {'iso-prolog-standard', 'prolog-interpreter'}\n",
      "tau-prolog/tau-prolog ciao-lang/ciao {'iso-prolog-standard', 'prolog-interpreter'}\n",
      "tau-prolog/tau-prolog mthom/scryer-prolog {'iso-prolog-standard', 'prolog-interpreter'}\n",
      "AppliedLogicSystems/ALSProlog ciao-lang/ciao {'iso-prolog-standard', 'programming-language', 'compiler'}\n",
      "AppliedLogicSystems/ALSProlog lambduli/minilog {'programming-language', 'language'}\n",
      "ciao-lang/ciao mthom/scryer-prolog {'iso-prolog-standard', 'prolog-interpreter'}\n",
      "ciao-lang/ciao lps-js/lps.js {'programming-language', 'interpreter'}\n",
      "AndreaInfUFSM/elc117-2024b jaalonso/Lecturas_GLC {'haskell', 'functional-programming'}\n",
      "twolodzko/prolog-rs mthom/scryer-prolog {'rust', 'prolog-interpreter'}\n",
      "cmungall/sparqlprog wouterbeek/prolog_rdf {'semantic-web', 'rdf'}\n",
      "GEXF file created: temp/logic_repos.gexf\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import ast\n",
    "from itertools import combinations\n",
    "\n",
    "similar_topics = ['swi-prolog',\n",
    " 'prolog-implementation',\n",
    " 'prolog',\n",
    " 'prolog-programming-language',\n",
    " 'logic-programming',\n",
    " 'logic',\n",
    " 'answer-set-programming',\n",
    " 'declarative-programming'\n",
    " 'datalog']\n",
    "\n",
    "df_repos = pd.read_csv(\"temp/logic_tag_repos.csv\")\n",
    "\n",
    "# Create an empty undirected graph.\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes: Each repo becomes a node, and its topics are stored as a node attribute.\n",
    "for idx, row in df_repos.iterrows():\n",
    "    repo = row['repo_name']\n",
    "    topics = row['topics']\n",
    "    # Ensure topics are properly parsed from string representation of lists\n",
    "    if isinstance(topics, str) and topics.startswith(\"[\") and topics.endswith(\"]\"):\n",
    "        topics = ast.literal_eval(topics)  # Convert string list to actual list\n",
    "    elif isinstance(topics, str):\n",
    "        topics = [topics]  # Convert single topic string to list\n",
    "    \n",
    "    topics_cleaned = [t.strip().lower() for t in topics]  # Clean topics\n",
    "    topics_str = ','.join(topics_cleaned)\n",
    "    G.add_node(repo, topics=topics_str)\n",
    "\n",
    "# Add edges: Check for shared tags between repos and add an edge with weight equal to shared topics count in similar_topics.\n",
    "repos = list(G.nodes)\n",
    "for repo1, repo2 in combinations(repos, 2):\n",
    "    topics1 = set(G.nodes[repo1]['topics'].split(','))\n",
    "    topics2 = set(G.nodes[repo2]['topics'].split(','))\n",
    "    shared_topics = topics1 & topics2  # Ensure lowercase comparison\n",
    "    relevant_shared_topics = shared_topics & set(similar_topics)\n",
    "    non_relevant_shared_topics = shared_topics - set(similar_topics)\n",
    "    if len(relevant_shared_topics)>=2 & len(non_relevant_shared_topics)>=1:\n",
    "        print(repo1, repo2, non_relevant_shared_topics)\n",
    "        G.add_edge(repo1, repo2)\n",
    "\n",
    "# Write the graph to a GEXF file.\n",
    "output_file = \"temp/logic_repos.gexf\"\n",
    "nx.write_gexf(G, output_file)\n",
    "print(f\"GEXF file created: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "234440b7-24fd-467a-88ae-696c84792430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEXF file created: temp/logic_topics.gexf\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import ast\n",
    "from itertools import combinations\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "df_repos = pd.read_csv(\"temp/logic_tag_repos.csv\")\n",
    "\n",
    "# Count topic frequencies\n",
    "topic_counter = Counter()\n",
    "topic_pairs = defaultdict(set)  # Dictionary to track topic pairs and the repos they appear in\n",
    "\n",
    "for idx, row in df_repos.iterrows():\n",
    "    topics = row['topics']\n",
    "    if isinstance(topics, str) and topics.startswith(\"[\") and topics.endswith(\"]\"):\n",
    "        topics = ast.literal_eval(topics)  # Convert string list to actual list\n",
    "    elif isinstance(topics, str):\n",
    "        topics = [topics]  # Convert single topic string to list\n",
    "    \n",
    "    topics_cleaned = {t.strip().lower() for t in topics}\n",
    "    topic_counter.update(topics_cleaned)\n",
    "    \n",
    "    for topic1, topic2 in combinations(topics_cleaned, 2):\n",
    "        topic_pairs[(topic1, topic2)].add(idx)  # Store the repo index where the pair appears\n",
    "\n",
    "# Filter topics with frequency greater than 5\n",
    "filtered_topics = {topic for topic, count in topic_counter.items() if count > 5}\n",
    "\n",
    "# Create an empty undirected graph.\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add edges between topics appearing in at least two same repositories and in the filtered set\n",
    "for (topic1, topic2), repos in topic_pairs.items():\n",
    "    if len(repos) >= 10 and topic1 in filtered_topics and topic2 in filtered_topics:\n",
    "        G.add_edge(topic1, topic2)\n",
    "\n",
    "# Add filtered topics as nodes\n",
    "G.add_nodes_from(filtered_topics)\n",
    "\n",
    "# Write the graph to a GEXF file.\n",
    "output_file = \"temp/logic_topics.gexf\"\n",
    "nx.write_gexf(G, output_file)\n",
    "print(f\"GEXF file created: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
